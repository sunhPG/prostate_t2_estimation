{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.transform import radon, iradon\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torchvision\n",
    "import PIL\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dicom\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cv2\n",
    "import skimage as skimage\n",
    "from skimage.measure import label \n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import nibabel as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        t1, t2, t1m, bladder_mask = sample['t1'], sample['t2'], sample['t1m'], sample['bladder_mask']\n",
    "        t1 = t1.astype('float64')  \n",
    "        t2 = t2.astype('float64')  \n",
    "        t1m = t1m.astype('float64') \n",
    "        bladder_mask = bladder_mask.astype('float64')\n",
    "        \n",
    "        \n",
    "        return {'t1': torch.from_numpy(t1),\n",
    "                't2': torch.from_numpy(t2),\n",
    "                't1m': torch.from_numpy(t1m),\n",
    "                'bladder_mask': torch.from_numpy(bladder_mask)\n",
    "                \n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Blocks in Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class resconv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(resconv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        residual =  self.Conv_1x1(x)\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        #print(residual.size())\n",
    "        #print(x.size())\n",
    "        \n",
    "        return residual+x\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "    \n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "        \n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(single_conv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self,ngpu,img_ch=2,output_ch=1):\n",
    "        super(U_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "   \n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "   \n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "       \n",
    "\n",
    "        # decoding + concat path\n",
    "\n",
    "        \n",
    "        d4 = self.Up4(x4)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Net Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function - delete th files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_file(path):\n",
    "    ls = os.listdir(path)\n",
    "    for i in ls:\n",
    "        c_path = os.path.join(path, i)\n",
    "        if os.path.isdir(c_path):\n",
    "            del_file(c_path)\n",
    "        else:\n",
    "            os.remove(c_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datapath Defination for Train, Validataion and Test: Train_, Val_, Test_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataDir = '/mnt/LiDXXLab_Files/Haoran/prostate_QM/training_dataset_3/elastic_resized/'\n",
    "ls = os.listdir(TrainDataDir)\n",
    "print(ls)\n",
    "print(len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['sub0125', 'sub0408', 'sub0419', 'sub0513', 'sub1006', 'sub0211', 'sub1216', 'sub1201', 'sub0901', 'sub0608', 'sub0106',  'sub0519', 'sub0427', 'sub0517', 'sub0413', 'sub0624', 'sub1117', 'sub0930', 'sub0526', 'sub0415', 'sub0316',  'sub0207', 'sub0518', 'sub0527', 'sub0412']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the subject idex to three groups.\n",
    "Total subjects = 20\n",
    "\n",
    "Train : Validation : Testing = 14 : 3 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project has air : 0517,0526,\n",
    "## project moving a lot : 05202\n",
    "\n",
    "\n",
    "split_number = 1\n",
    "train_idex = ['sub0519', 'sub0419', 'sub1201', 'sub0930', 'sub0527', 'sub0412', 'sub0608', 'sub1006', 'sub0125', 'sub0211', 'sub0207', 'sub0316', 'sub0513', 'sub0624', 'sub0427', 'sub0901', 'sub0408', 'sub0415', 'sub0518']\n",
    "\n",
    "testing_idex = ['sub1216', 'sub0413','sub1117','sub0106']\n",
    "\n",
    "validation_idex = ['sub0517','sub0526']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_number = 2\n",
    "train_idex = ['sub0519', 'sub0106', 'sub0419', 'sub1201', 'sub0930', 'sub0527', 'sub0412', 'sub0413', 'sub0608', 'sub1117', 'sub1006', 'sub0125', 'sub0207', 'sub1216', 'sub0513', 'sub0624', 'sub0408', 'sub0415', 'sub0518']\n",
    "\n",
    "testing_idex = ['sub0427','sub0211','sub0901','sub0316']\n",
    "\n",
    "validation_idex = ['sub0517','sub0526']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_number = 3\n",
    "train_idex = ['sub0519', 'sub0106', 'sub1201', 'sub0930', 'sub0527', 'sub0412', 'sub0413', 'sub0608', 'sub1117', 'sub0125', 'sub0211', 'sub0207', 'sub1216', 'sub0316', 'sub0513', 'sub0427', 'sub0901', 'sub0408', 'sub0518']\n",
    "\n",
    "testing_idex = ['sub0419','sub1006','sub0415','sub0624']\n",
    "\n",
    "validation_idex = ['sub0517','sub0526']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_number = 4\n",
    "train_idex = ['sub0519', 'sub0106', 'sub0419', 'sub0527', 'sub0412', 'sub0413', 'sub1117', 'sub1006', 'sub0211', 'sub1216', 'sub0316', 'sub0513', 'sub0624', 'sub0427', 'sub0901', 'sub0408', 'sub0415', 'sub0518']\n",
    "\n",
    "testing_idex = ['sub1201','sub0207','sub0125','sub0930','sub0608']\n",
    "\n",
    "validation_idex = ['sub0517','sub0526']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject_idex = ls\n",
    "testing_idex = []\n",
    "validation_idex = []\n",
    "\n",
    "for subject_elements in subject_idex:\n",
    "    #print(subject_elements)\n",
    "    if subject_elements.startswith(\"sub0901\"):\n",
    "        testing_idex.append(subject_elements)\n",
    "    if subject_elements.startswith(\"sub0930\"):\n",
    "        validation_idex.append(subject_elements)\n",
    "train_idex =   list(set(subject_idex)-set(testing_idex)-set(validation_idex))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_number = len(train_idex)\n",
    "validation_dataset_number = len(validation_idex)\n",
    "testing_dataset_number= len(testing_idex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_number)\n",
    "print(validation_dataset_number)\n",
    "print(testing_dataset_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Defination for Train, Validataion and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "class MRIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, DataDir,ImgPrefix,transform=None):\n",
    "        print('***************')\n",
    "        print('MRIDataset')\n",
    "        imgprefix = ImgPrefix[0]\n",
    "        print(ImgPrefix)\n",
    "        print('---------------')\n",
    "                       \n",
    "        AllT1 = []\n",
    "        AllT2 = []\n",
    "        AllT1map = []\n",
    "        AllMaskbladder = []\n",
    "        AllMasktissue = []\n",
    "        \n",
    "        T1Img = nb.load(DataDir+imgprefix+'/t1_elastic_resized.nii')\n",
    "        T2Img = nb.load(DataDir+imgprefix+'/t2_elastic_resized.nii')\n",
    "        T1mapImg = nb.load(DataDir+imgprefix+'/t2m_fitted_elastic_resized.nii')\n",
    "        AllMask_bladder = nb.load(DataDir+'bladder_mask_t2m/'+imgprefix+'_bladder_mask_t2m.nii')\n",
    "        #AllMask_tissue = nb.load(DataDir+'elastic_resize_tissuemask/'+imgprefix+'_tissue_outline.nii')\n",
    "        \n",
    "        AllT1 = T1Img.get_fdata()\n",
    "        AllT2 = T2Img.get_fdata()\n",
    "        AllT1map = T1mapImg.get_fdata()\n",
    "        AllMaskbladder = AllMask_bladder.get_fdata()\n",
    "        #AllMasktissue = AllMask_tissue.get_fdata()\n",
    "        \n",
    "        AllMaskbladder_idex = np.asarray(AllMaskbladder)\n",
    "                \n",
    "        T1_norm = np.asarray(AllT1[AllMaskbladder_idex >=1]).mean()+3*np.std(AllT1[AllMaskbladder_idex >=1]).astype(np.float32)\n",
    "        T2_norm = np.asarray(AllT2[AllMaskbladder_idex >=1]).mean()+3*np.std(AllT2[AllMaskbladder_idex >=1]).astype(np.float32)\n",
    "        T1m_norm = 400\n",
    "        \n",
    "        print(T1_norm)\n",
    "        print(T2_norm)\n",
    "        \n",
    "        AllT1 = AllT1/T1_norm\n",
    "        AllT2 = AllT2/T2_norm\n",
    "        AllT1map = AllT1map/T1m_norm\n",
    "        \n",
    "        \n",
    "        print(imgprefix+' loaded...')\n",
    "        print(np.shape(AllT1))\n",
    "        print(np.shape(AllT2))\n",
    "        print(np.shape(AllT1map))\n",
    "        \n",
    "        for imgprefix in ImgPrefix[1:len(ImgPrefix)]:\n",
    "            \n",
    "            T1Img = nb.load(DataDir+imgprefix+'/t1_elastic_resized.nii')\n",
    "            T2Img = nb.load(DataDir+imgprefix+'/t2_elastic_resized.nii')\n",
    "            T1mapImg = nb.load(DataDir+imgprefix+'/t2m_fitted_elastic_resized.nii')\n",
    "            MaskImg_bladder = nb.load(DataDir+'bladder_mask_t2m/'+imgprefix+'_bladder_mask_t2m.nii')\n",
    "            #MaskImg_tissue = nb.load(DataDir+'elastic_resize_tissuemask/'+imgprefix+'_tissue_outline.nii')\n",
    "            \n",
    "            \n",
    "            T1Data = T1Img.get_fdata()\n",
    "            T2Data = T2Img.get_fdata()\n",
    "            T1mapData = T1mapImg.get_fdata()\n",
    "            MaskData_bladder = MaskImg_bladder.get_fdata()\n",
    "            #MaskData_tissue = MaskImg_tissue.get_fdata()\n",
    "            \n",
    "            MaskData_bladder_index = np.asarray(MaskData_bladder)\n",
    "            \n",
    "            T1_norm = np.asarray(T1Data[MaskData_bladder_index >=1]).mean()+3*np.std(T1Data[MaskData_bladder_index >=1]).astype(np.float32)\n",
    "            T2_norm = np.asarray(T2Data[MaskData_bladder_index >=1]).mean()+3*np.std(T2Data[MaskData_bladder_index >=1]).astype(np.float32)\n",
    "            T1m_norm = 400\n",
    "            \n",
    "            T1Data = T1Data/T1_norm\n",
    "            T2Data = T2Data/T2_norm\n",
    "            T1mapData = T1mapData/T1m_norm\n",
    "            \n",
    "            print(imgprefix+' loaded...')\n",
    "           \n",
    "            AllT1 = np.concatenate((AllT1, T1Data), axis=2)\n",
    "            AllT2 = np.concatenate((AllT2, T2Data), axis=2)\n",
    "            AllT1map = np.concatenate((AllT1map, T1mapData), axis=2)\n",
    "            AllMaskbladder = np.concatenate((AllMaskbladder, MaskData_bladder), axis=2)\n",
    "            #AllMasktissue = np.concatenate((AllMasktissue, MaskData_tissue), axis=2)\n",
    "\n",
    "        \n",
    "        AllT1 = np.transpose(AllT1, (2, 0, 1))\n",
    "        AllT2 = np.transpose(AllT2, (2, 0, 1))\n",
    "        AllT1map = np.transpose(AllT1map, (2, 0, 1))\n",
    "        AllMaskbladder = np.transpose(AllMaskbladder, (2, 0, 1))\n",
    "        #AllMasktissue = np.transpose(AllMasktissue, (2, 0, 1))\n",
    "\n",
    "        print('Total MRIDataset size is: ' + str(np.shape(AllT1)))\n",
    "        print('Total MRIDataset size is: ' + str(np.shape(AllMaskbladder)))\n",
    "        #print('Total MRIDataset size is: ' + str(np.shape(AllMasktissue)))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.AllT1 = AllT1\n",
    "        self.AllT2 = AllT2\n",
    "        self.AllT1map = AllT1map\n",
    "        self.AllMaskbladder = AllMaskbladder\n",
    "        #self.AllMasktissue = AllMasktissue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.AllT1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        sample = {'t1': np.array(self.AllT1[idx]),\n",
    "                  't2': np.array(self.AllT2[idx]),\n",
    "                  't1m': np.array(self.AllT1map[idx]),\n",
    "                  'bladder_mask': np.array(self.AllMaskbladder[idx])\n",
    "                  #'tissue_mask': np.array(self.AllMasktissue[idx]) \n",
    "                  }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataloader for train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "Train_MRIDataset = MRIDataset(DataDir=TrainDataDir,ImgPrefix=train_idex,\n",
    "                        transform=transforms.Compose([ToTensor()]))\n",
    "Train_dataloader = DataLoader(Train_MRIDataset, batch_size=4,\n",
    "                       shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "Validation_MRIDataset = MRIDataset(DataDir=TrainDataDir,ImgPrefix=validation_idex,\n",
    "                                 transform=transforms.Compose([ToTensor()]))\n",
    "Val_dataloader = DataLoader(Validation_MRIDataset, batch_size=4,\n",
    "                       shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Test_MRIDataset = MRIDataset(DataDir=TrainDataDir,ImgPrefix=testing_idex,\n",
    "                                  transform=transforms.Compose([ToTensor()]))\n",
    "Test_dataloader = DataLoader(Test_MRIDataset, batch_size=1,\n",
    "                             shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPU setup. (add multiple GPU code...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setup for Trainning and Validation\n",
    "ngpu = 2\n",
    "TEST = 0\n",
    "num_epochs = 120\n",
    "lr = 1e-3 # lr: learning rate, can try big to small\n",
    "beta1 = 0.9\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Net Selection and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Selection\n",
    "U_Net = U_Net(ngpu).to(device)\n",
    "\n",
    "# Net Initialization\n",
    "if TEST:\n",
    "    U_Net.load_state_dict(torch.load(Setting+'/BackProjector.pt'))\n",
    "    U_Net.eval()\n",
    "else:\n",
    "    U_Net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Optimization Method Selection (i.e., loss function, regularization function, strategy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "optimizerUnet = optim.Adam(U_Net.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Trainng and Validation: save the Train_Loss, Val_Loss and save the Model_Best and Model_Last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date = now.strftime(\"%Y%m%d\")\n",
    "print(\"date and time:\",date)\n",
    "\n",
    "ModelPath_Best = '/mnt/LiDXXLab_Files/Haoran/prostate_QM/trianing_model/sub_25_cv/t2m_fitted/Best_model/Unet_mean3stdnorm_exblader'+'_'+date\n",
    "\n",
    "ModelPath_Last = '/mnt/LiDXXLab_Files/Haoran/prostate_QM/trianing_model/sub_25_cv/t2m_fitted/Last_model/Unet_mean3stdnorm_exblader'+'_'+date\n",
    "\n",
    "ModelPath_Inter = '/mnt/LiDXXLab_Files/Haoran/prostate_QM/trianing_model/sub_25_cv/t2m_fitted/Inter_model/Unet_mean3stdnorm_exblader'+'_'+date\n",
    "\n",
    "os.makedirs(ModelPath_Best, exist_ok = True)\n",
    "os.makedirs(ModelPath_Last, exist_ok = True)\n",
    "os.makedirs(ModelPath_Inter, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_train_avg_before=1000;\n",
    "Err_val_avg_before=1000;\n",
    "\n",
    "Err_train_avg_list = []\n",
    "Err_val_avg_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    Err_train_total = 0\n",
    "    Err_val_total = 0\n",
    "    Err_train_avg = 0\n",
    "    Err_val_avg = 0 \n",
    "\n",
    "    Train_iter=0\n",
    "    U_Net.train()\n",
    "    for i, data in enumerate(Train_dataloader, 0):\n",
    "        \n",
    "        t1 = data['t1'].to(device, dtype=torch.float).requires_grad_()\n",
    "        t2 = data['t2'].to(device, dtype=torch.float).requires_grad_()\n",
    "        t1m = data['t1m'].to(device, dtype=torch.float).requires_grad_()\n",
    "        bladder_mask = data['bladder_mask'].to(device, dtype=torch.float).requires_grad_()\n",
    "       \n",
    "        \n",
    "        \n",
    "        bladder_mask_idex = bladder_mask.cpu().detach().numpy()\n",
    "       \n",
    "        \n",
    "        t1m_idex = t1m.cpu().detach().numpy()\n",
    "        t1m_mask = t1m_idex.copy()\n",
    "        t1m_mask[t1m_idex>1]=0\n",
    "        t1m_mask[t1m_idex<=1]=1\n",
    "           \n",
    "       \n",
    "        bladder_mask_invert = bladder_mask_idex.copy()\n",
    "        bladder_mask_invert = np.abs(bladder_mask_invert-1)\n",
    "    \n",
    "        mask_loss = bladder_mask_invert*t1m_mask\n",
    "        mask_loss = torch.tensor(mask_loss)\n",
    "       \n",
    "    \n",
    "        Prediction = torch.squeeze(U_Net(torch.stack([t1, t2], dim=1)))\n",
    "        \n",
    "        del t1,t2\n",
    "        \n",
    "        Prediction_vector = Prediction[mask_loss == 1]\n",
    "        del Prediction\n",
    "        \n",
    "        Target_vector = t1m[mask_loss == 1]\n",
    "        del t1m, mask_loss\n",
    "        \n",
    "        optimizerUnet.zero_grad()\n",
    "        Err_train = criterion(Prediction_vector, Target_vector)\n",
    "        Err_train.backward()#backward\n",
    "        optimizerUnet.step()#forward\n",
    "\n",
    "        #if i % 50 == 0:\n",
    "        #    print('Err_train')\n",
    "        #    print(Err_train.item())\n",
    "            \n",
    "        Err_train_total += Err_train.item()\n",
    "        Train_iter += 1 \n",
    "        \n",
    "    Err_train_avg=Err_train_total/train_dataset_number # Haoran please change here\n",
    "    Err_train_avg_list.append(Err_train_avg)\n",
    "    \n",
    "    # Validation\n",
    "    Val_iter=0\n",
    "    \n",
    "    U_Net.eval()\n",
    "    for i, data in enumerate(Val_dataloader, 0):\n",
    "        \n",
    "        t1 = data['t1'].to(device, dtype=torch.float).requires_grad_()\n",
    "        t2 = data['t2'].to(device, dtype=torch.float).requires_grad_()\n",
    "        t1m = data['t1m'].to(device, dtype=torch.float).requires_grad_()\n",
    "        bladder_mask = data['bladder_mask'].to(device, dtype=torch.float).requires_grad_()\n",
    "       \n",
    "        \n",
    "        \n",
    "        bladder_mask_idex = bladder_mask.cpu().detach().numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        t1m_idex = t1m.cpu().detach().numpy()\n",
    "        t1m_mask = t1m_idex.copy()\n",
    "        t1m_mask[t1m_idex>1]=0\n",
    "        t1m_mask[t1m_idex<=1]=1\n",
    "       \n",
    "        \n",
    "        bladder_mask_invert = bladder_mask_idex.copy()\n",
    "        bladder_mask_invert = np.abs(bladder_mask_invert-1)\n",
    "    \n",
    "        mask_loss = bladder_mask_invert*t1m_mask\n",
    "        mask_loss = torch.tensor(mask_loss)\n",
    "        \n",
    "        Prediction = torch.squeeze(U_Net(torch.stack([t1, t2], dim=1)))\n",
    "        del t1,t2\n",
    "        \n",
    "        Prediction_vector = Prediction[mask_loss == 1]\n",
    "        del Prediction\n",
    "        \n",
    "        Target_vector = t1m[mask_loss == 1]\n",
    "        del t1m,  mask_loss\n",
    "        \n",
    "        Err_val = criterion(Prediction_vector, Target_vector)\n",
    "        \n",
    "        #if i % 10 == 0:\n",
    "        #    print('Err_val')\n",
    "        #    print(Err_val.item())\n",
    "            \n",
    "        Err_val_total += Err_val.item()\n",
    "        Val_iter += 1 \n",
    "        \n",
    "    Err_val_avg=Err_val_total/validation_dataset_number \n",
    "    Err_val_avg_list.append(Err_val_avg)\n",
    "    \n",
    "    U_Net.train()\n",
    "    \n",
    "    ## Save the best\n",
    "    # check if Err_train decreases and if Err_val decreases\n",
    "    if Err_train_avg < Err_train_avg_before and Err_val_avg < Err_val_avg_before:\n",
    "        del_file(ModelPath_Best)\n",
    "        torch.save(U_Net.state_dict(), ModelPath_Best+'/sub_25_Unet_CVsplit'+str(split_number)+'_epoch'+str(epoch)+'_'+date+'.pt')\n",
    "        model_best_epoch = epoch\n",
    "        Err_train_avg_before = Err_train_avg\n",
    "        Err_val_avg_before = Err_val_avg\n",
    "        print('[%d/%d]\\t Err_val_before: %.10f\\t'\n",
    "              % (epoch, num_epochs, Err_val_avg_before))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('[%d/%d]\\t Err_train_avg: %.10f\\t Err_val_avg: %.10f\\t'\n",
    "              % (epoch, num_epochs, Err_train_avg,Err_val_avg))\n",
    "        \n",
    "        \n",
    "    if epoch % 10 == 0 and epoch > 20:\n",
    "        torch.save(U_Net.state_dict(), ModelPath_Inter+'/sub_25_Unet_CVsplit'+str(split_number)+'_epoch'+str(epoch)+'_'+date+'.pt')\n",
    "        \n",
    "# save the Model_Last\n",
    "torch.save(U_Net.state_dict(), ModelPath_Last+'/sub_25_Unet_CVsplit'+str(split_number)+'_epoch'+str(epoch)+'_'+date+'.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  7.2 Plot Train and Validation Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot errors for all epoches\n",
    "plt.plot(Err_train_avg_list,'b')\n",
    "plt.plot(Err_val_avg_list,'r')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "#plt.axis([0, num_epochs, 0, 0.04])\n",
    "plt.show()\n",
    "print(model_best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot errors for all epoches\n",
    "plt.plot(Err_train_avg_list,'b')\n",
    "plt.plot(Err_val_avg_list,'r')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlim([0, 100])\n",
    "plt.show()\n",
    "print(model_best_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
